{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAHHca5l2QRd"
   },
   "source": [
    "# Lab06 - NLP2 - Encoder-decoder model\n",
    "## Using the pyTorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA3BfljP19mM"
   },
   "source": [
    "#### install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy sacrebleu torchdata -U -q\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeefPbcJ11YO"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w2_g9nG4zMsm"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTIAQR033AHa"
   },
   "source": [
    "Create source and target language tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B9rppaYl13l5"
   },
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgyabLwBYs2d"
   },
   "source": [
    "#### Seq2Seq Network using Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JyogvtWJ3D42"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF2GR4K9YxjO"
   },
   "source": [
    "During training, we need a subsequent word mask that will prevent the model from looking into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eR8fS6rrYn1f"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generate a square subsequent mask.\n",
    "\n",
    "    Args:\n",
    "        sz (int): Size of the mask.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Generated square subsequent mask.\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src: Tensor, tgt: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Create masks for source and target sequences.\n",
    "\n",
    "    Args:\n",
    "        src (Tensor): Source sequence tensor.\n",
    "        tgt (Tensor): Target sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: Tuple of masks:\n",
    "            - src_mask: Source mask tensor.\n",
    "            - tgt_mask: Target mask tensor.\n",
    "            - src_padding_mask: Source padding mask tensor.\n",
    "            - tgt_padding_mask: Target padding mask tensor.\n",
    "    \"\"\"\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up_Fvrp-Y879"
   },
   "source": [
    "Let’s now define the parameters of our model and instantiate the same. Below, we also define our loss function which is the cross-entropy loss and the optimizer used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m35_TO28Y6NP"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkTVR9XQZIoz"
   },
   "source": [
    "#### Collation\n",
    "\n",
    "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings. We need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that can be fed directly into our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IkGqx7x0ZCKG"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operationsd\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch: List[Tuple[str, str]]) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for batching and padding sequences.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple[str, str]]): List of tuples containing source and target sequences.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Tuple of padded source and target batches.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P88MkAMFZOha"
   },
   "source": [
    "Let’s define training and evaluation loop that will be called for each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qXvxt4zMZL9S"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be trained.\n",
    "        optimizer: The optimizer used for training.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss value for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be evaluated.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss value on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX0gyBqrZSkA"
   },
   "source": [
    "Now we have all the ingredients to train our model. Let’s do it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CT_FWMLwZQC5",
    "outputId": "49dd93ad-1a64-4247-bd6d-caa64e9cfbf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.344, Val loss: 4.114, Epoch time = 45.463s\n",
      "Epoch: 2, Train loss: 3.760, Val loss: 3.320, Epoch time = 43.257s\n",
      "Epoch: 3, Train loss: 3.161, Val loss: 2.895, Epoch time = 43.165s\n",
      "Epoch: 4, Train loss: 2.768, Val loss: 2.639, Epoch time = 45.398s\n",
      "Epoch: 5, Train loss: 2.480, Val loss: 2.443, Epoch time = 43.580s\n",
      "Epoch: 6, Train loss: 2.251, Val loss: 2.318, Epoch time = 44.649s\n",
      "Epoch: 7, Train loss: 2.061, Val loss: 2.201, Epoch time = 43.906s\n",
      "Epoch: 8, Train loss: 1.897, Val loss: 2.112, Epoch time = 43.450s\n",
      "Epoch: 9, Train loss: 1.754, Val loss: 2.061, Epoch time = 44.349s\n",
      "Epoch: 10, Train loss: 1.631, Val loss: 2.002, Epoch time = 43.780s\n",
      "Epoch: 11, Train loss: 1.524, Val loss: 1.969, Epoch time = 44.530s\n",
      "Epoch: 12, Train loss: 1.419, Val loss: 1.942, Epoch time = 43.457s\n",
      "Epoch: 13, Train loss: 1.334, Val loss: 1.968, Epoch time = 44.681s\n",
      "Epoch: 14, Train loss: 1.252, Val loss: 1.944, Epoch time = 43.750s\n",
      "Epoch: 15, Train loss: 1.173, Val loss: 1.933, Epoch time = 44.604s\n",
      "Epoch: 16, Train loss: 1.103, Val loss: 1.922, Epoch time = 43.435s\n",
      "Epoch: 17, Train loss: 1.039, Val loss: 1.899, Epoch time = 44.937s\n",
      "Epoch: 18, Train loss: 0.979, Val loss: 1.906, Epoch time = 43.470s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "def greedy_decode(model, src: Tensor, src_mask: Tensor, max_len: int, start_symbol: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generate the output sequence using the greedy algorithm.\n",
    "\n",
    "    Args:\n",
    "        model: The model used for decoding.\n",
    "        src (Tensor): Source sequence tensor.\n",
    "        src_mask (Tensor): Source mask tensor.\n",
    "        max_len (int): Maximum length of the output sequence.\n",
    "        start_symbol (int): Start symbol index.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The generated output sequence.\n",
    "    \"\"\"\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Translate an input sentence into the target language.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The translation model.\n",
    "        src_sentence (str): The input sentence to be translated.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated sentence.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pJHyeaWN-rvG"
   },
   "outputs": [],
   "source": [
    "torch.save(transformer, 'saved_transformer.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5oi6baRl-zQI"
   },
   "outputs": [],
   "source": [
    "saved_transformer = torch.load('saved_transformer.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xwH_NmbZVZf",
    "outputId": "36899047-cf59-43d6-c509-69c9d990aaef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of people standing in front of an igloo . \n"
     ]
    }
   ],
   "source": [
    "print(translate(saved_transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxCwnQU4Zy5s"
   },
   "source": [
    "## Theoretical questions\n",
    "\n",
    "##### **1.In the positional encoding, why are we using a combination of sinus and cosinus?**\n",
    "\n",
    "In positional encoding, a combination of sine and cosine functions is used to represent the position of each word in the input sequence. This is done to capture both the relative and absolute positions of the words. The sinusoidal functions provide different frequencies that encode different positions along the sequence. By using a combination of sine and cosine functions, the positional encoding can represent different positions with unique patterns, allowing the model to differentiate between words at different positions in the sequence.\n",
    "\n",
    "##### **2. In the Seq2SeqTransformer class, What is the parameter nhead for? What is the point of the generator?**\n",
    "\n",
    "- The parameter \"nhead\" stands for the number of attention heads. In the transformer model, multi-head attention is used to capture different types of dependencies between words in the input sequence. Each attention head attends to different parts of the sequence and learns different patterns of relationships. Increasing the number of attention heads allows the model to capture more complex dependencies and enhance its ability to focus on different aspects of the input during attention calculation.\n",
    "\n",
    "- The \"generator\" is responsible for generating the output sequence based on the decoder's hidden state. It takes the decoder's hidden state as input and applies a linear transformation followed by a softmax activation to produce the probability distribution over the vocabulary. The generator essentially predicts the next word in the output sequence.\n",
    "\n",
    "\n",
    "##### **3.Describe the goal of the create_mask function. Why does it handle differently the source and target masks?**\n",
    "\n",
    "The goal of the create_mask function is to create attention masks for the source and target sequences in the transformer model. The masks are used during the attention calculation to ensure that the model attends only to the relevant parts of the sequences.\n",
    "The function handles the source and target masks differently because they serve different purposes in the model:\n",
    "\n",
    "- Source mask: The source mask is used in the encoder to prevent attending to future positions in the source sequence.This ensures that the encoder only attends to the positions that have been already processed and avoids any information leakage from future positions.\n",
    "\n",
    "- Target mask: The target mask is used in the decoder during both the self-attention and encoder-decoder attention calculations. It serves two purposes. Firstly, it prevents attending to future positions, similar to the source mask. Secondly, it also masks out the padding positions in the target sequence so that the model does not attend to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsDK7ce5fMln"
   },
   "source": [
    "## Decoding functions\n",
    "\n",
    "- A top-k sampling with temperature for decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YpXXi04UZYf0"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def topk_sample(logits: Tensor, k: int, temperature: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Perform top-k sampling with temperature on the logits.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Logits from the model output.\n",
    "        k (int): Number of candidates to consider.\n",
    "        temperature (float): Temperature value for scaling logits.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Top-k value.\n",
    "\n",
    "    \"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    topk_values, topk_indices = torch.topk(scaled_logits, k=k, dim=-1)\n",
    "    probabilities = F.softmax(topk_values, dim=-1)\n",
    "    sampled_token = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    next_token = topk_indices[:, sampled_token].squeeze()\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def topk_temperature_decode(\n",
    "    model,\n",
    "    src: Tensor,\n",
    "    src_mask: Tensor,\n",
    "    max_len: int,\n",
    "    start_symbol: int,\n",
    "    k: int,\n",
    "    temperature: float\n",
    ") -> Tensor:\n",
    "  \"\"\"\n",
    "    Generate output sequence using greedy algorithm with top-k sampling and temperature.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        src (Tensor): Source input.\n",
    "        src_mask (Tensor): Source input mask.\n",
    "        max_len (int): Maximum length of the output sequence.\n",
    "        start_symbol (int): Start symbol for decoding.\n",
    "        k (int): Number of candidates to consider for top-k sampling.\n",
    "        temperature (float): Temperature value for scaling logits during sampling.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Generated output sequence.\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "  src = src.to(DEVICE)\n",
    "  src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "  memory = model.encode(src, src_mask)\n",
    "  ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "  for i in range(max_len-1):\n",
    "      memory = memory.to(DEVICE)\n",
    "      tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                  .type(torch.bool)).to(DEVICE)\n",
    "      out = model.decode(ys, memory, tgt_mask)\n",
    "      out = out.transpose(0, 1)\n",
    "      prob = model.generator(out[:, -1])\n",
    "      next_word = topk_sample(prob, k=k, temperature=temperature)\n",
    "      ys = torch.cat([ys,\n",
    "                      torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "      if next_word == EOS_IDX:\n",
    "          break\n",
    "  return ys\n",
    "\n",
    "def translate_topk_temperature(\n",
    "    model: nn.Module,\n",
    "    src_sentence: str,\n",
    "    k: int,\n",
    "    temperature: float\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate an input sentence into the target language using top k temperature decoding\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained translation model.\n",
    "        src_sentence (str): Input sentence to be translated.\n",
    "        k (int): Number of candidates to consider for top-k sampling.\n",
    "        temperature (float): Temperature value for scaling logits during sampling.\n",
    "\n",
    "    Returns:\n",
    "        str: Translated sentence in the target language.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = topk_temperature_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQWLkIsgkmmG"
   },
   "source": [
    "- A top-p sampling with temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8YUqPe8yknPF"
   },
   "outputs": [],
   "source": [
    "def top_p_sample(logits: Tensor, p: float, temperature: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Perform top-p sampling with temperature on the logits.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Logits from the model output.\n",
    "        p (float): Cumulative probability threshold.\n",
    "        temperature (float): Temperature value for scaling logits.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Sampled token.\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)\n",
    "    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    nucleus = cum_sum_probs < p\n",
    "    nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)\n",
    "    sorted_log_probs = torch.log(sorted_probs)\n",
    "\n",
    "    sorted_log_probs[~nucleus] = float('-inf')\n",
    "    sampled_indices = indices[nucleus]\n",
    "    sampled_token = torch.multinomial(sorted_log_probs.exp(), num_samples=1)\n",
    "    next_token = sampled_indices[sampled_token].squeeze()\n",
    "    return next_token\n",
    "\n",
    "\n",
    "    sorted_log_probs[~nucleus] = float('-inf')\n",
    "    sampled_sorted_indexes = torch.max(sorted_log_probs)\n",
    "    print(sampled_sorted_indexes)\n",
    "    \n",
    "    res = indices.gather(-1, sampled_sorted_indexes.unsqueeze(-1))\n",
    "    return res.squeeze(-1)\n",
    "\n",
    "def topp_temperature_decode(\n",
    "    model,\n",
    "    src: Tensor,\n",
    "    src_mask: Tensor,\n",
    "    max_len: int,\n",
    "    start_symbol: int,\n",
    "    p: float,\n",
    "    temperature: float,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Generate output sequence using greedy algorithm with top-p sampling and temperature.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        src (Tensor): Source input.\n",
    "        src_mask (Tensor): Source input mask.\n",
    "        max_len (int): Maximum length of the output sequence.\n",
    "        start_symbol (int): Start symbol for decoding.\n",
    "        p (float): Cumulative probability threshold for top-p sampling.\n",
    "        temperature (float): Temperature value for scaling logits during sampling.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Generated output sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (\n",
    "            generate_square_subsequent_mask(ys.size(0))\n",
    "            .type(torch.bool)\n",
    "            .to(DEVICE)\n",
    "        )\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = top_p_sample(prob, p=p, temperature=temperature)\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0\n",
    "        )\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate_topp_temperature(\n",
    "    model: nn.Module,\n",
    "    src_sentence: str,\n",
    "    p: int,\n",
    "    temperature: float\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate an input sentence into the target language using top k temperature decoding\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained translation model.\n",
    "        src_sentence (str): Input sentence to be translated.\n",
    "        k (int): Number of candidates to consider for top-k sampling.\n",
    "        temperature (float): Temperature value for scaling logits during sampling.\n",
    "\n",
    "    Returns:\n",
    "        str: Translated sentence in the target language.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = topp_temperature_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le9PqwGKlV1R"
   },
   "source": [
    "### Compare Translation by playing with the k, p and temperature parameters\n",
    "\n",
    "We know that: \n",
    "\n",
    "- The **k parameter** determines the number of candidates considered for sampling. It controls the size of the set of words from which the final word is sampled.\n",
    "\n",
    "- The **temperature parameter** controls the softmax temperature during sampling.\n",
    "\n",
    "- The **p parameter** determines the cumulative probability threshold for top-p sampling. It controls the size of the set of words considered for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wzdn4jvildHE",
    "outputId": "3819195d-fdff-4f1e-9485-4fec7d68d708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP K DECODER:\n",
      "case k=3, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
      "case k=3, temperature=0.8 ->  A group of people standing in front of an igloo . \n",
      "case k=3, temperature=1.0 ->  A group of people standing in front of an auditorium . \n",
      "case k=3, temperature=1.2 ->  A group of people stand in front of an igloo . \n",
      "case k=5, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
      "case k=5, temperature=0.8 ->  A group of people standing in front of an igloo \n",
      "case k=5, temperature=1.0 ->  A group of people stand in front of an igloo . \n",
      "case k=5, temperature=1.2 ->  A group of people are standing in front of an igloo . \n",
      "case k=10, temperature=0.6 ->  A group of people stand in front of an igloo . \n",
      "case k=10, temperature=0.8 ->  A group of people stand in front of an auditorium . \n",
      "case k=10, temperature=1.0 ->  A group of people standing in front of an anvil . \n",
      "case k=10, temperature=1.2 ->  A group of people standing in front of an auditorium . \n",
      "case k=50, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
      "case k=50, temperature=0.8 ->  A group of people standing in front of an igloo \n",
      "case k=50, temperature=1.0 ->  A group of people are standing outside an axe . \n",
      "case k=50, temperature=1.2 ->  A group of people stand in front of an orchestra . \n",
      "\n",
      "TOP P DECODER:\n",
      "case p=0.1, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
      "case p=0.1, temperature=0.8 ->  A group of people standing in front of an igloo . \n",
      "case p=0.1, temperature=1.0 ->  A group of people standing in front of an industrial office . \n",
      "case p=0.1, temperature=1.2 ->  A group of people standing in front of an irish studio . \n",
      "case p=0.3, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
      "case p=0.3, temperature=0.8 ->  A group of people standing in front of an irish room . \n",
      "case p=0.3, temperature=1.0 ->  A group of people standing in front of an language . \n",
      "case p=0.3, temperature=1.2 ->  A group of people standing in front of an OR . \n",
      "case p=0.8, temperature=0.6 ->  A group of people stand in front of an auditorium . \n",
      "case p=0.8, temperature=0.8 ->  A group of people stand in front of an award \n",
      "case p=0.8, temperature=1.0 ->  A group of people standing in front of an hearth . \n",
      "case p=0.8, temperature=1.2 ->  A group of people standing in front of an alotment . \n",
      "case p=0.9, temperature=0.6 ->  A group of people stand in front of an outboard Mazda . \n",
      "case p=0.9, temperature=0.8 ->  A group of people stand in front of an bitten . \n",
      "case p=0.9, temperature=1.0 ->  A group of people standing in front of an advocacy courtyard . \n",
      "case p=0.9, temperature=1.2 ->  A group of people stand in front of an abandoned weather . \n"
     ]
    }
   ],
   "source": [
    "k_s = [3, 5, 10, 50]\n",
    "p_s = [0.1, 0.3, 0.8, 0.9]\n",
    "temperatures = [0.6, 0.8, 1.0, 1.2]\n",
    "\n",
    "print(\"TOP K DECODER:\")\n",
    "for k in k_s:\n",
    "  for temperature in temperatures:\n",
    "    translation = translate_topk_temperature(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k, temperature)\n",
    "    print(f\"case k={k}, temperature={temperature} -> {translation}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TOP P DECODER:\")\n",
    "for p in p_s:\n",
    "  for temperature in temperatures:\n",
    "    translation = translate_topp_temperature(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p, temperature)\n",
    "    print(f\"case p={p}, temperature={temperature} -> {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3UOJsLFPQ0t"
   },
   "source": [
    "According to the result we can say that:\n",
    "\n",
    "- A **larger value of k** includes more candidates, increasing the diversity of potential samples. A **smaller value of k** restricts the candidates to a smaller subset, leading to more deterministic samples\n",
    "\n",
    "-  A **higher temperature (>1)** softens the probability distribution, making lower probability words more likely to be sampled. This introduces more randomness and diversity. Conversely, a **lower temperature (<1)** sharpens the distribution, making high probability words more likely to be sampled, reducing randomness.\n",
    "\n",
    "-  A **larger value of p** includes a larger portion of the probability mass of the probability distribution, resulting in a more diverse sample. A **smaller value of p** focuses on a narrower set of high-probability words, leading to more focused and deterministic samples.\n",
    "\n",
    "- **Greedy decoding** always get the word with the highest probability. For this reason, leading to deterministic samples.\n",
    "\n",
    "\n",
    "\n",
    "#### To conclude:\n",
    "\n",
    "The choice of k (topk case) or p (topp case) and temperature depends on the specific task and desired outcomes. Here are some considerations:\n",
    "\n",
    "- For **more diverse and exploratory outputs**, higher values of k (topk case) or p (topp case) and higher values of temperature can be used.\n",
    "\n",
    "- If you want **more controlled and focused outputs**, lower values of k (topk case) or p (topp case) and lower values of temperature can be used.\n",
    "\n",
    "The challenge is to experiment with different combinations of k (topk case) or p (topp case) and temperature to find the right balance between diversity and control in the generated samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_M9L84RVO1h"
   },
   "source": [
    "## Compute the BLEU score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zMsPeAGzTRlQ"
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = Multi30k(root='.data', split=('train', 'valid', 'test'), language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "\n",
    "\n",
    "# Define parameters for topk and topp samplings\n",
    "temperature=0.8\n",
    "p = 0.1\n",
    "k = 3\n",
    "\n",
    "# Evaluate the model using sacreBLEU\n",
    "greedy_translated_outputs = []\n",
    "topk_translated_outputs = []\n",
    "topp_translated_outputs = []\n",
    "reference_translations = []\n",
    "\n",
    "\n",
    "for example in valid_dataset:\n",
    "    src_sentence = example[0]\n",
    "    tgt_sentence = example[1]\n",
    "\n",
    "    # Add greedy translation\n",
    "    greedy_translated_outputs.append(translate(transformer, src_sentence))\n",
    "\n",
    "    # Add topk translation\n",
    "    topk_translated_outputs.append(translate_topk_temperature(transformer, src_sentence, k, temperature))\n",
    "\n",
    "    # Add topp translation\n",
    "    topp_translated_outputs.append(translate_topp_temperature(transformer, src_sentence, p, temperature))\n",
    "\n",
    "    # Add expected translation\n",
    "    reference_translations.append([tgt_sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VV1AfQUSaXsW",
    "outputId": "eef6925f-000c-4a39-8ef9-83dc53bbb70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREEDY DECODING:\n",
      "BLEU Score: 44.17918226831576\n",
      "\n",
      "topk DECODING:\n",
      "BLEU Score: 44.17918226831576\n",
      "\n",
      "topp DECODING:\n",
      "BLEU Score: 38.260294162784476\n"
     ]
    }
   ],
   "source": [
    "bleu = BLEU()\n",
    "\n",
    "# Getting score for greedy decoding\n",
    "greedy_bleu = bleu.corpus_score(greedy_translated_outputs, reference_translations)\n",
    "\n",
    "# Print the output values for greedy decoding\n",
    "print(\"GREEDY DECODING:\")\n",
    "print(\"BLEU Score:\", greedy_bleu.score)\n",
    "print()\n",
    "\n",
    "# Getting score for topk decoding\n",
    "topk_bleu = bleu.corpus_score(topk_translated_outputs, reference_translations)\n",
    "\n",
    "# Print the output values for greedy decoding\n",
    "print(\"top-k DECODING:\")\n",
    "print(\"BLEU Score:\", topk_bleu.score)\n",
    "print()\n",
    "\n",
    "# Getting score for topp decoding\n",
    "topp_bleu = bleu.corpus_score(topp_translated_outputs, reference_translations)\n",
    "\n",
    "# Print the output values for greedy decoding\n",
    "print(\"top-p DECODING:\")\n",
    "print(\"BLEU Score:\", topp_bleu.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W47HVXHQjg_p"
   },
   "source": [
    "### Fine-tuning parameters\n",
    "#### For top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6T0OqSKTcxmi",
    "outputId": "34f89c7e-1f36-43ff-c0f0-f7c1862c1f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.8\n",
      "k: 5\n",
      "BLEU Score: 48.76836638685217\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.8\n",
      "k: 10\n",
      "BLEU Score: 43.86913376508308\n",
      "\n",
      "Temperature: 0.8\n",
      "k: 20\n",
      "BLEU Score: 45.38407873007615\n",
      "\n",
      "Temperature: 0.9\n",
      "k: 5\n",
      "BLEU Score: 50.81327481546147\n",
      "\n",
      "Temperature: 0.9\n",
      "k: 10\n",
      "BLEU Score: 60.427507947135354\n",
      "\n",
      "Temperature: 0.9\n",
      "k: 20\n",
      "BLEU Score: 54.91004867761124\n",
      "\n",
      "Temperature: 1.0\n",
      "k: 5\n",
      "BLEU Score: 35.93041119630842\n",
      "\n",
      "Temperature: 1.0\n",
      "k: 10\n",
      "BLEU Score: 54.60241725418134\n",
      "\n",
      "Temperature: 1.0\n",
      "k: 20\n",
      "BLEU Score: 47.369534260115934\n",
      "\n",
      "Best Parameters:\n",
      "Temperature: 0.9\n",
      "k: 10\n",
      "Best BLEU Score: 60.427507947135354\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "temperatures = [0.8, 0.9, 1.0]\n",
    "k_values = [5, 10, 20]\n",
    "\n",
    "best_score = 0.0\n",
    "best_params = {}\n",
    "\n",
    "# Perform hyperparameter search\n",
    "for temperature in temperatures:\n",
    "  for k in k_values:\n",
    "    translated_outputs = []\n",
    "    reference_translations = []\n",
    "    for example in test_dataset:\n",
    "        src_sentence = example[0]\n",
    "        tgt_sentence = example[1]\n",
    "        \n",
    "        # Generate the translation with specified hyperparameters\n",
    "        translation = translate_topk_temperature(transformer, src_sentence, k, temperature)\n",
    "        translated_outputs.append(translation)\n",
    "        reference_translations.append([tgt_sentence])\n",
    "\n",
    "    # Compute sacreBLEU score\n",
    "    bleu = sacrebleu.corpus_bleu(translated_outputs, reference_translations)\n",
    "\n",
    "    # Print the hyperparameters and corresponding BLEU score\n",
    "    print(\"Temperature:\", temperature)\n",
    "    print(\"k:\", k)\n",
    "    print(\"BLEU Score:\", bleu.score)\n",
    "    print()\n",
    "\n",
    "    # Update best score and best parameters if a higher score is achieved\n",
    "    if bleu.score > best_score:\n",
    "        best_score = bleu.score\n",
    "        best_params = {\n",
    "            'temperature': temperature,\n",
    "            'k': k,\n",
    "        }\n",
    "\n",
    "# Print the best parameters and corresponding BLEU score\n",
    "print(\"Best Parameters:\")\n",
    "print(\"Temperature:\", best_params['temperature'])\n",
    "print(\"k:\", best_params['k'])\n",
    "print(\"Best BLEU Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERLLTlfDkYsk",
    "outputId": "9bb6c50e-d2a5-4864-c9bf-c43ad1695fbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.8\n",
      "p: 0.8\n",
      "BLEU Score: 47.7189707581088\n",
      "\n",
      "Temperature: 0.8\n",
      "p: 0.9\n",
      "BLEU Score: 46.92470064105599\n",
      "\n",
      "Temperature: 0.8\n",
      "p: 1.0\n",
      "BLEU Score: 48.326978309062206\n",
      "\n",
      "Temperature: 0.9\n",
      "p: 0.8\n",
      "BLEU Score: 57.067457770560026\n",
      "\n",
      "Temperature: 0.9\n",
      "p: 0.9\n",
      "BLEU Score: 47.037095938668955\n",
      "\n",
      "Temperature: 0.9\n",
      "p: 1.0\n",
      "BLEU Score: 72.59795291154772\n",
      "\n",
      "Temperature: 1.0\n",
      "p: 0.8\n",
      "BLEU Score: 53.07712171072446\n",
      "\n",
      "Temperature: 1.0\n",
      "p: 0.9\n",
      "BLEU Score: 56.234132519034915\n",
      "\n",
      "Temperature: 1.0\n",
      "p: 1.0\n",
      "BLEU Score: 46.17366309441026\n",
      "\n",
      "Best Parameters:\n",
      "Temperature: 0.9\n",
      "p: 1.0\n",
      "Best BLEU Score: 72.59795291154772\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "p_values = [0.8, 0.9, 1.0]\n",
    "\n",
    "best_score = 0.0\n",
    "best_params = {}\n",
    "\n",
    "# Perform hyperparameter search\n",
    "for temperature in temperatures:\n",
    "  for p in p_values:\n",
    "    translated_outputs = []\n",
    "    reference_translations = []\n",
    "    for example in test_dataset:\n",
    "        src_sentence = example[0]\n",
    "        tgt_sentence = example[1]\n",
    "        \n",
    "        # Generate the translation with specified hyperparameters\n",
    "        translation = translate_topp_temperature(transformer, src_sentence, p, temperature)\n",
    "        translated_outputs.append(translation)\n",
    "        reference_translations.append([tgt_sentence])\n",
    "\n",
    "    # Compute sacreBLEU score\n",
    "    bleu = sacrebleu.corpus_bleu(translated_outputs, reference_translations)\n",
    "\n",
    "    # Print the hyperparameters and corresponding BLEU score\n",
    "    print(\"Temperature:\", temperature)\n",
    "    print(\"p:\", p)\n",
    "    print(\"BLEU Score:\", bleu.score)\n",
    "    print()\n",
    "\n",
    "    # Update best score and best parameters if a higher score is achieved\n",
    "    if bleu.score > best_score:\n",
    "        best_score = bleu.score\n",
    "        best_params = {\n",
    "            'temperature': temperature,\n",
    "            'p': p,\n",
    "        }\n",
    "\n",
    "# Print the best parameters and corresponding BLEU score\n",
    "print(\"Best Parameters:\")\n",
    "print(\"Temperature:\", best_params['temperature'])\n",
    "print(\"p:\", best_params['p'])\n",
    "print(\"Best BLEU Score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
